{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Query  \\\n",
      "0  How can I reset my password?   \n",
      "1    What is the refund policy?   \n",
      "2      How do I track my order?   \n",
      "\n",
      "                                            Response  \n",
      "0  To reset your password, click on \"Forgot Passw...  \n",
      "1  Our refund policy lasts 30 days. To initiate a...  \n",
      "2  You can track your order using the tracking nu...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Simulated dataset\n",
    "data = {\n",
    "    'Query': ['How can I reset my password?', 'What is the refund policy?', 'How do I track my order?'],\n",
    "    'Response': ['To reset your password, click on \"Forgot Password\" on the login page.',\n",
    "                 'Our refund policy lasts 30 days. To initiate a return, contact support.',\n",
    "                 'You can track your order using the tracking number provided in your email.']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\smart\\anaconda3\\envs\\smartai_customersupport\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\smart\\anaconda3\\envs\\smartai_customersupport\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 41.57 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 125.06 examples/s]\n",
      "100%|██████████| 3/3 [04:28<00:00, 89.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 268.8979, 'train_samples_per_second': 0.033, 'train_steps_per_second': 0.011, 'train_loss': 7.201426823933919, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=7.201426823933919, metrics={'train_runtime': 268.8979, 'train_samples_per_second': 0.033, 'train_steps_per_second': 0.011, 'total_flos': 2351670755328.0, 'train_loss': 7.201426823933919, 'epoch': 3.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Set the padding token to the end-of-sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Query'], padding='max_length', truncation=True)\n",
    "\n",
    "# Create a Dataset object\n",
    "dataset = Dataset.from_pandas(df)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare the labels\n",
    "tokenized_dataset = tokenized_dataset.map(lambda examples: {'labels': examples['input_ids']}, batched=True)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Fine-tuning setup\n",
    "training_args = TrainingArguments(output_dir=\"./results\", num_train_epochs=3, per_device_train_batch_size=4)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Creation and Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\smart\\anaconda3\\envs\\smartai_customersupport\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load Sentence-BERT model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for the queries\n",
    "embeddings = model.encode(df['Query'].tolist())\n",
    "\n",
    "# Store embeddings in a simulated vector database (dictionary)\n",
    "vector_database = {}\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    vector_database[i] = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Retrieval-Augmented Generation (RAG) System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To reset your password, click on \"Forgot Password\" on the login page.\n"
     ]
    }
   ],
   "source": [
    "def retrieve_similar(query, vector_database, model):\n",
    "    query_embedding = model.encode([query])\n",
    "    similarities = cosine_similarity(query_embedding, list(vector_database.values()))\n",
    "    most_similar = np.argmax(similarities)\n",
    "    return most_similar\n",
    "\n",
    "test_cases = \"\"\"\n",
    "I forgot my password, how do I reset it?\n",
    "How can I change my account password?\n",
    "What is your return policy?\n",
    "Where can I find my order details?\n",
    "\"\"\" \n",
    "query = \"I forgot my password, how do I reset it?\"\n",
    "retrieved_index = retrieve_similar(query, vector_database, model)\n",
    "response = df['Response'][retrieved_index]\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
